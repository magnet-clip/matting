# Interactive Video Matting

## Подход
Я немного допилил SAM2 чтобы он кроме сегментации возвращал еще и промежуточные данные. 
Далее, была сделана довольно простая UNet подобная сеть, которая принимает на вход
 - исходное изображение
 - сегментацию и промежуточные данные SAM2
И возвращает маттинг

## Обучение
Для обучения использовался датасет VideoMatte240. 
Структура обучения:

1. Подготовка исходных данных (скрипт `dataset.ipynb`)
    - из датасета формировался набор обучающих видео - то есть, на папку с фреймами выбирался случайный фон, изображеня накладывались на фон с помощью маттинга, полученный кадр копировался в новую папку
    - далее, после получения около 200 таких тренировочных видео, для каждого из них выполнялсь разметка при помощи SAM2:
     - в первом кадре выбиралось несколько случайных точек 
     - выполнялся прогон SAM2 по папке и для каждого видео сохранялись сегментация и файлы с промежуточными данными
    - затем, полученные папки с разметкой отсматривались вручную, чтобы убрать некачественные примеры. Сегментация работала плохо в ряде случаев, например когда:
        - есть посторонние предметы, например телефон, сумка, зонт
        - есть шапки, наушники, шлемы
        - в кадре несколько людей
        - в первом кадре нет человека
        - кто-то появляется в середине видео

2. Само обучение (скрипт `train.ipynb`)

    Обучение было стандартным. Основные моменты:
    - данные: у меня было много однообразных данных (в видео из датасета разница между кадрами довольно невелика). Я обнаружил, что качество обучения не ухудшается, если использовать только каждый 10-ый кадр. Из хорошего тут то, что скорость обучения выросла в 10 раз, а из плохого - получилось, что у меня только около 2000 фреймов для обучения.
    - лосс: сначала я использовал обычный l1 лосс, однако качество получалось плохим. По сути, я восстанавливал сегментацию, а зоны маттинга не получали достаточно внимания. Поэтому я изменил лосс и стал давать 10х премию за лосс в зоне, где ground truth показывает полупрозрачность. Это заметно улучшило качество.
    - аугментации: я использовал случайные отражения и упругие деформации. Вероятно, стоило бы также использовать случайные гомографии, не хватило времени попробовать.

3. Базовая проверка  (скрипт `matting.ipynb`)
    Просто подгрузка модели и применение ее в режиме инференса. Использовать очень неудобно, поэтому сделал тестовое приложени (см раздел приложение ниже). 



## Проблемы

Основными проблемами было:
 - определение, как должна была быть устроена модель, и что должно пойти на вход модели. Не хотелось в процессе обучения гонять SAM, поэтому потратил время на то, чтобы подготовить и отсмотреть датасет с промежуточными данными из SAM.
 - поиск баланса между скоростью обучения и сложностью модели. Потратил слишком много времени на сложные модели, надо было начинать с таких, что обучались бы очень быстро.

 Менее существенное:
 - не использую негативные точки
 - по-хорошему, следовало бы немного усложнить работу с SAM - первый кадр я сегментирую в режиме единичного изображения, а остальные в режиме видео, и качество получается заметно лучше. Надо было бы сделать все в режиме видео
 - недостаток места и времени для подготовки данных (там всего около 500 видео, однако времени и места на диске хватило на примерно 200 видео). 

 Не попробовал:
  - LORA / файнтюнинг самого SAM2
  - Улучшенные варианты SAM2, например Samurai


**Еще попробовать - брать сигму от сегментации и добавить гомографии, считать метрики!**


## Приложение

Разработка и тестирование производились на машине с Ubuntu 24.04 и видеокартой 4060 c 16 гигабайтами памяти

### Установка и настройка

### Бэк

Для установки зависимостей использовал `uv`:
 * Установка `uv`: https://docs.astral.sh/uv/getting-started/installation/
 * Установка зависимостей:
   - `cd backend`
   - `uv venv`
   - `uv sync --frozen -vv`
   - Установка sam2: один из вариантов ниже
      - `uv install uv pip install https://github.com/magnet-clip/sam2.git` - может не сработать, у `uv` бывают какие-то проблемы с cuda
      - `source ./venv/bin/activate && pip install https://github.com/magnet-clip/sam2.git` - должно сработать

Запуск: `uv run ./server.py`

Можно попробовать через REST клиент Bruno (https://www.usebruno.com/). В папке `rest` есть коллекция запросов. Обратите внимание: для открытия коллекции в бруно надо выбирать не конкретный файл, а всю папку `rest`


### Фронт
Сделан на Solid (https://www.solidjs.com/) - похож на реакт, но позволяет более детальную настройку реактивности

 * Установить `pnpm`: https://pnpm.io/installation
 * `cd frontend`
 * `pnpm install`
 * `pnpm run start`
 * Зайти на  http://localhost:3000/

Использование фронта должно быть достаточно интуитивно. Слева можно добавить видео (пробовал на небольших mp4 файлах). После добавления можно на нем нажать, откроется редактор

В редакторе можно ходить по фреймам, ставить точки. Потом при нажатии кнопки Matting надо указать начальный и конечный кадры. В начальном кадре должны быть точки, конечный не должен выходить за пределы видео.

После выполнения маттинга в этом же диалоге можно скачать архив (закрыв диалог скачать уже не получится)

После закрытия диалога маттинг должен отрисоваться поверх фреймов. Можно посмотреть, как получилось.
Проект: Interactive Video Matting

Основная идея: создание мультимодальной модели интерактивного матирования (matting) видео на основе модели Segment Anything 2. Решением задачи матирования является предсказание альфа-канала (прозрачности) в каждом пикселе для реалистичного удаления или замены фона.

Это исследовательский проект, вы попробуете разобраться в нескольких научных статьях, запустить базовую модель SAM 2, выполнить дообучение этой модели под новую задачу на новых данных и замерить субъективно и объективно достигнутые метрики качества полученной модели. Также от вас ожидается собранный гитахб и несложное веб приложения для тестирования.


Предлагаю проект построить следующим образом:

1. Изучение статей SAM 2 и Matting Anything

2. Запустить Base модель

3. Найти и скачать датасеты матирования (примеры датасетов - 1, 2, 3, 4)

4. Сделать fine-tuning с помощью PEFT/LORA-адаптера базовой модели на новую задачу (замена Loss функции, замена последнего слоя с сегментации на регрессию)

5. Сделать демо десктоп/веб-приложение с возможность загрузки тестового видео, совершения кликов на целевой объект, а также до получения выходного видео с целевым объектом с помощью разработанной модели.

6. Посчитать объективные метрики качества дообученной модели и базовой на задаче матирования, опционально можно сравнится по метрикам с SOTA методами матирования, например, из статьи RVM. В качестве модальностей для этого тестирования достаточно рассмотреть клики, генератор кликов можно взять из задачи интерактивной сегментации.
Метрики
https://github.com/SHI-Labs/Matting-Anything/blob/main/evaluation/metrics.py

В начале января (до 8) предлагаю устроить созвон и обсудить, получилось ли понять задачу и что с ней делать, а также поделиться промежуточными результатами (по желанию).